/**
 * OpenTelemetry tracing for AI Agent Ollama calls.
 * Emits gen_ai.* spans following the OTel GenAI Semantic Conventions
 * so Dynatrace AI Observability can visualize LLM requests, tokens, latency.
 *
 * Env vars (set by the app or .env):
 *   DT_ENVIRONMENT   â€” e.g. https://abc12345.live.dynatrace.com
 *   DT_PLATFORM_TOKEN â€” API token with openTelemetryTrace.ingest scope
 *
 * The OTLP exporter sends to: {DT_ENVIRONMENT}/api/v2/otlp/v1/traces
 */

import { NodeTracerProvider } from '@opentelemetry/sdk-trace-node';
import { BatchSpanProcessor } from '@opentelemetry/sdk-trace-node';
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-proto';
import { Resource } from '@opentelemetry/resources';
// @ts-ignore â€” Resource constructor varies across OTel SDK versions
const createResource = (attrs: Record<string, string>) => new (Resource as any)(attrs);
import { trace, context, SpanKind, SpanStatusCode, Span } from '@opentelemetry/api';
import { createLogger } from './logger.js';

const log = createLogger('otel');

// â”€â”€â”€ Types â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

export interface GenAISpanOptions {
  operation: string;                  // 'chat' | 'chatJSON' | 'agentLoop'
  model: string;
  messages: { role: string; content: string }[];
  temperature?: number;
  tools?: { function: { name: string } }[];
}

export interface GenAISpanResult {
  content: string;
  toolCalls?: number;
  totalDurationMs: number;
  promptTokens?: number;
  completionTokens?: number;
  error?: string;
}

// â”€â”€â”€ Singleton Provider â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

let _provider: NodeTracerProvider | null = null;
let _initialized = false;

function getDTEndpoint(): string | null {
  const env = process.env.DT_ENVIRONMENT || process.env.DYNATRACE_URL || '';
  if (!env) return null;
  // Strip trailing slash, ensure no .apps. in URL
  const base = env.replace(/\/+$/, '').replace('.apps.dynatrace', '.dynatrace');
  return `${base}/api/v2/otlp/v1/traces`;
}

function getDTToken(): string | null {
  return process.env.DT_PLATFORM_TOKEN || process.env.DYNATRACE_TOKEN || process.env.DT_API_TOKEN || null;
}

export function initTracing(): void {
  if (_initialized) return;
  _initialized = true;

  const endpoint = getDTEndpoint();
  const token = getDTToken();

  if (!endpoint || !token) {
    log.warn('âš ï¸  OTel tracing NOT enabled â€” DT_ENVIRONMENT or DT_PLATFORM_TOKEN not set');
    log.info('ðŸ“‹ Current environment variables:');
    log.info(`   DT_ENVIRONMENT: ${process.env.DT_ENVIRONMENT ? 'âœ“ set' : 'âœ— not set'}`);
    log.info(`   DYNATRACE_URL: ${process.env.DYNATRACE_URL ? 'âœ“ set' : 'âœ— not set'}`);
    log.info(`   DT_PLATFORM_TOKEN: ${process.env.DT_PLATFORM_TOKEN ? 'âœ“ set (hidden)' : 'âœ— not set'}`);
    log.info(`   DYNATRACE_TOKEN: ${process.env.DYNATRACE_TOKEN ? 'âœ“ set (hidden)' : 'âœ— not set'}`);
    log.info(`   DT_API_TOKEN: ${process.env.DT_API_TOKEN ? 'âœ“ set (hidden)' : 'âœ— not set'}`);
    log.info('ðŸ’¡ To enable AI Observability: set DT_ENVIRONMENT and DT_PLATFORM_TOKEN');
    return;
  }

  log.info(`âœ… Initializing OTel tracing for AI Observability`);
  log.info(`   Endpoint: ${endpoint}`);
  log.info(`   Service: bizobs-ai-agents v1.0.0`);

  const resource = createResource({
    'service.name': 'bizobs-ai-agents',
    'service.version': '1.0.0',
    'deployment.environment': process.env.NODE_ENV || 'production',
  });

  const exporter = new OTLPTraceExporter({
    url: endpoint,
    headers: {
      Authorization: `Api-Token ${token}`,
    },
  });

  _provider = new NodeTracerProvider({
    resource,
    spanProcessors: [new BatchSpanProcessor(exporter)],
  });
  _provider.register();

  log.info('ðŸŽ¯ OTel tracing initialized â€” GenAI spans for Ollama calls will appear in Dynatrace');
  log.info('ðŸ“Š View in Dynatrace: Notebooks > Davis AI > AI Observability');
}

export function getTracer() {
  return trace.getTracer('bizobs-ai-agents', '1.0.0');
}

// â”€â”€â”€ GenAI Span Wrapper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/**
 * Wrap an Ollama call in a GenAI-convention span.
 * Uses attributes from https://docs.dynatrace.com/docs/observe/dynatrace-for-ai-observability/models-and-platforms/ollama
 */
export async function withGenAISpan<T>(
  options: GenAISpanOptions,
  fn: () => Promise<T>,
  extractResult?: (result: T) => GenAISpanResult,
): Promise<T> {
  if (!_provider) {
    // Tracing not enabled â€” run without span
    return fn();
  }

  const tracer = getTracer();
  const spanName = `${options.operation} ${options.model}`;

  return tracer.startActiveSpan(spanName, {
    kind: SpanKind.CLIENT,
    attributes: {
      // GenAI semantic conventions
      'gen_ai.system': 'ollama',
      'gen_ai.request.model': options.model,
      'llm.request.type': options.operation === 'chatJSON' ? 'chat' : options.operation,
      'gen_ai.request.temperature': options.temperature ?? 0.3,

      // Prompt content (first user + system messages)
      ...(options.messages[0] ? {
        'gen_ai.prompt.0.role': options.messages[0].role,
        'gen_ai.prompt.0.content': truncate(options.messages[0].content, 4096),
      } : {}),
      ...(options.messages.length > 1 ? {
        'gen_ai.prompt.1.role': options.messages[1].role,
        'gen_ai.prompt.1.content': truncate(options.messages[1].content, 4096),
      } : {}),

      // Tool info
      ...(options.tools?.length ? {
        'gen_ai.request.tools_count': options.tools.length,
        'gen_ai.request.tools': options.tools.map(t => t.function.name).join(', '),
      } : {}),
    },
  }, async (span: Span) => {
    try {
      const result = await fn();

      if (extractResult) {
        const r = extractResult(result);
        span.setAttributes({
          'gen_ai.completion.0.role': 'assistant',
          'gen_ai.completion.0.content': truncate(r.content, 4096),
          'gen_ai.response.model': options.model,
          ...(r.promptTokens != null ? { 'gen_ai.usage.prompt_tokens': r.promptTokens } : {}),
          ...(r.completionTokens != null ? { 'gen_ai.usage.completion_tokens': r.completionTokens } : {}),
          ...(r.toolCalls ? { 'gen_ai.response.tool_calls': r.toolCalls } : {}),
          'gen_ai.response.duration_ms': r.totalDurationMs,
        });

        span.setStatus({ code: SpanStatusCode.OK });
      } else {
        span.setStatus({ code: SpanStatusCode.OK });
      }

      span.end();
      return result;
    } catch (err) {
      const errMsg = err instanceof Error ? err.message : String(err);
      span.setStatus({ code: SpanStatusCode.ERROR, message: errMsg });
      span.setAttribute('error.message', errMsg);
      span.setAttribute('error.type', err instanceof Error ? err.constructor.name : 'Error');
      span.end();
      throw err;
    }
  });
}

// â”€â”€â”€ Graceful shutdown â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

export async function shutdownTracing(): Promise<void> {
  if (_provider) {
    log.info('Shutting down OTel tracing...');
    await _provider.shutdown();
  }
}

// â”€â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

function truncate(s: string, max: number): string {
  return s.length <= max ? s : s.substring(0, max) + '...[truncated]';
}
